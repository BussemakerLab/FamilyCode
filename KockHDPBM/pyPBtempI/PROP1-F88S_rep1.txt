### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.3983743191 Reg.: 0.0016847369 Distance: 0.6673221588 Patience: 10
			Epoch 1 took 0.01s NLL: 1.3983743191 Reg.: 0.0016847369 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.25084114074707
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.860278606414795
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.950908184051514
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.56s NLL: 1.8089430332 Reg.: 0.0010618045 Distance: 24.5671138763 Patience: 10
			Loss decreased
			Epoch 1 took 0.70s NLL: 1.4412020445 Reg.: 0.0005271942 Distance: 14.9498453140 Patience: 10
			Loss decreased
			Epoch 2 took 0.56s NLL: 1.3046871424 Reg.: 0.0004866674 Distance: 2.5381116867 Patience: 10
			Loss decreased
			Epoch 3 took 0.55s NLL: 1.3046405315 Reg.: 0.0004582481 Distance: 0.7155621648 Patience: 10
			Loss decreased
			Epoch 4 took 0.43s NLL: 1.3041261435 Reg.: 0.0000944572 Distance: 14.2580852509 Patience: 10
			Epoch 5 took 0.23s NLL: 1.3041261435 Reg.: 0.0000944572 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.860278606414795
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.950908184051514
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-2.7558e-01, -4.3617e-01, -5.7889e-01,  2.8082e-03, -5.8015e-01,
			                -4.6118e-01, -6.0116e-01,  3.5362e-01, -3.9382e-01, -9.7724e-01,
			                -3.7722e-01,  4.6144e-01,  1.0894e+00, -3.7202e+00,  4.6678e-01,
			                 8.7662e-01,  3.9437e-01, -9.2295e-01, -2.2073e-01, -5.3499e-01,
			                -2.1308e+00,  1.2877e+00, -2.1283e+00,  1.6833e+00, -2.1477e-01,
			                -9.6160e-01, -1.1922e-01,  7.1783e-03,  3.9640e-02, -5.1270e-01,
			                -3.8987e-01, -4.2620e-01]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.860278606414795
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.950908184051514
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-2.7558e-01, -4.3617e-01, -5.7889e-01,  2.8082e-03, -5.8015e-01,
			                -4.6118e-01, -6.0116e-01,  3.5362e-01, -3.9382e-01, -9.7724e-01,
			                -3.7722e-01,  4.6144e-01,  1.0894e+00, -3.7202e+00,  4.6678e-01,
			                 8.7662e-01,  3.9437e-01, -9.2295e-01, -2.2073e-01, -5.3499e-01,
			                -2.1308e+00,  1.2877e+00, -2.1283e+00,  1.6833e+00, -2.1477e-01,
			                -9.6160e-01, -1.1922e-01,  7.1783e-03,  3.9640e-02, -5.1270e-01,
			                -3.8987e-01, -4.2620e-01])
			Loss decreased
			Epoch 0 took 0.60s NLL: 1.2962723970 Reg.: 0.0000894139 Distance: 2.0081160069 Patience: 10
			Loss decreased
			Epoch 1 took 0.52s NLL: 1.2943829298 Reg.: 0.0000806558 Distance: 1.9351420403 Patience: 10
			Loss decreased
			Epoch 2 took 0.51s NLL: 1.2931432724 Reg.: 0.0000748360 Distance: 1.5782207251 Patience: 10
			Loss decreased
			Epoch 3 took 0.57s NLL: 1.2910752296 Reg.: 0.0000832693 Distance: 4.0676188469 Patience: 10
			Loss decreased
			Epoch 4 took 0.59s NLL: 1.2863676548 Reg.: 0.0000830272 Distance: 2.2357738018 Patience: 10
			Loss decreased
			Epoch 5 took 0.55s NLL: 1.2861034870 Reg.: 0.0000880708 Distance: 0.7769018412 Patience: 10
			Loss decreased
			Epoch 6 took 0.24s NLL: 1.2860940695 Reg.: 0.0000896740 Distance: 0.2655816376 Patience: 10
			Epoch 7 took 0.06s NLL: 1.2860940695 Reg.: 0.0000896376 Distance: 0.0060717869 Patience: 9
			Loss decreased
			Epoch 8 took 0.09s NLL: 1.2860940695 Reg.: 0.0000894735 Distance: 0.0192543808 Patience: 10
			Epoch 9 took 0.19s NLL: 1.2860940695 Reg.: 0.0000894735 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.224658966064453
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.959563255310059
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.3715, -0.5429, -0.3476, -0.2821, -0.4742, -0.3161, -0.4471, -0.3077,
			                -0.3693, -0.3320, -0.7044, -0.1375,  0.4353, -0.4254, -0.7198, -0.8339,
			                 0.5535, -0.8092, -0.7729, -0.5122, -0.1782, -0.6994, -1.0356,  0.3689,
			                 0.2166, -0.0929, -2.4284,  0.7600,  2.5871, -4.0479,  0.5971, -0.6816]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.3983743191 Reg.: 0.0016847369 Distance: 0.6673221588 Patience: 10
			Epoch 1 took 0.01s NLL: 1.3983743191 Reg.: 0.0016847369 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.25084114074707
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.860278606414795
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.950908184051514
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.55s NLL: 1.8089430332 Reg.: 0.0010618045 Distance: 24.5671138763 Patience: 10
			Loss decreased
			Epoch 1 took 0.72s NLL: 1.4412020445 Reg.: 0.0005271942 Distance: 14.9498453140 Patience: 10
			Loss decreased
			Epoch 2 took 0.54s NLL: 1.3046871424 Reg.: 0.0004866674 Distance: 2.5381116867 Patience: 10
			Loss decreased
			Epoch 3 took 0.52s NLL: 1.3046405315 Reg.: 0.0004582481 Distance: 0.7155621648 Patience: 10
			Loss decreased
			Epoch 4 took 0.42s NLL: 1.3041261435 Reg.: 0.0000944572 Distance: 14.2580852509 Patience: 10
			Epoch 5 took 0.21s NLL: 1.3041261435 Reg.: 0.0000944572 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.860278606414795
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.950908184051514
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-2.7558e-01, -4.3617e-01, -5.7889e-01,  2.8082e-03, -5.8015e-01,
			                -4.6118e-01, -6.0116e-01,  3.5362e-01, -3.9382e-01, -9.7724e-01,
			                -3.7722e-01,  4.6144e-01,  1.0894e+00, -3.7202e+00,  4.6678e-01,
			                 8.7662e-01,  3.9437e-01, -9.2295e-01, -2.2073e-01, -5.3499e-01,
			                -2.1308e+00,  1.2877e+00, -2.1283e+00,  1.6833e+00, -2.1477e-01,
			                -9.6160e-01, -1.1922e-01,  7.1783e-03,  3.9640e-02, -5.1270e-01,
			                -3.8987e-01, -4.2620e-01]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.860278606414795
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.950908184051514
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-2.7558e-01, -4.3617e-01, -5.7889e-01,  2.8082e-03, -5.8015e-01,
			                -4.6118e-01, -6.0116e-01,  3.5362e-01, -3.9382e-01, -9.7724e-01,
			                -3.7722e-01,  4.6144e-01,  1.0894e+00, -3.7202e+00,  4.6678e-01,
			                 8.7662e-01,  3.9437e-01, -9.2295e-01, -2.2073e-01, -5.3499e-01,
			                -2.1308e+00,  1.2877e+00, -2.1283e+00,  1.6833e+00, -2.1477e-01,
			                -9.6160e-01, -1.1922e-01,  7.1783e-03,  3.9640e-02, -5.1270e-01,
			                -3.8987e-01, -4.2620e-01])
			Loss decreased
			Epoch 0 took 0.60s NLL: 1.2962723970 Reg.: 0.0000894139 Distance: 2.0081160069 Patience: 10
			Loss decreased
			Epoch 1 took 0.53s NLL: 1.2943829298 Reg.: 0.0000806558 Distance: 1.9351420403 Patience: 10
			Loss decreased
			Epoch 2 took 0.54s NLL: 1.2931432724 Reg.: 0.0000748360 Distance: 1.5782207251 Patience: 10
			Loss decreased
			Epoch 3 took 0.60s NLL: 1.2910752296 Reg.: 0.0000832693 Distance: 4.0676188469 Patience: 10
			Loss decreased
			Epoch 4 took 0.60s NLL: 1.2863676548 Reg.: 0.0000830272 Distance: 2.2357738018 Patience: 10
			Loss decreased
			Epoch 5 took 0.54s NLL: 1.2861034870 Reg.: 0.0000880708 Distance: 0.7769018412 Patience: 10
			Loss decreased
			Epoch 6 took 0.24s NLL: 1.2860940695 Reg.: 0.0000896740 Distance: 0.2655816376 Patience: 10
			Epoch 7 took 0.06s NLL: 1.2860940695 Reg.: 0.0000896376 Distance: 0.0060717869 Patience: 9
			Loss decreased
			Epoch 8 took 0.09s NLL: 1.2860940695 Reg.: 0.0000894735 Distance: 0.0192543808 Patience: 10
			Epoch 9 took 0.20s NLL: 1.2860940695 Reg.: 0.0000894735 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.224658966064453
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.959563255310059
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.3715, -0.5429, -0.3476, -0.2821, -0.4742, -0.3161, -0.4471, -0.3077,
			                -0.3693, -0.3320, -0.7044, -0.1375,  0.4353, -0.4254, -0.7198, -0.8339,
			                 0.5535, -0.8092, -0.7729, -0.5122, -0.1782, -0.6994, -1.0356,  0.3689,
			                 0.2166, -0.0929, -2.4284,  0.7600,  2.5871, -4.0479,  0.5971, -0.6816]) 

