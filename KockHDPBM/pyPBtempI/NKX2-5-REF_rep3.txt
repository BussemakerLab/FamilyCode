### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.4885566235 Reg.: 0.0016843065 Distance: 0.6163921356 Patience: 10
			Epoch 1 took 0.01s NLL: 1.4885566235 Reg.: 0.0016843065 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.199911117553711
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.45s NLL: 2.2355046272 Reg.: 0.0010595312 Distance: 24.6246643066 Patience: 10
			Epoch 1 took 0.06s NLL: 2.2355046272 Reg.: 0.0010594969 Distance: 0.0006899763 Patience: 9
			Loss decreased
			Epoch 2 took 0.08s NLL: 2.2355046272 Reg.: 0.0010591871 Distance: 0.0057509737 Patience: 10
			Epoch 3 took 0.86s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
			                nan, nan, nan, nan, nan, nan, nan, nan])
			Epoch 0 took 0.60s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -3.2061,  -3.1892,  -3.2997,  -3.2825,  -3.4276,  -3.0225,  -3.4049,
			                 -3.1225,  -6.2083,  -6.2446,  -6.2835,   5.7589,   1.3265, -17.6486,
			                  1.7717,   1.5729,   5.6839,  -6.2524,  -6.2322,  -6.1767,  -9.8770,
			                  3.3343,  -9.7757,   3.3410,  -3.1535,  -3.2670,  -3.2227,  -3.3343,
			                 -3.3936,  -3.1126,  -3.1875,  -3.2837]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -3.2061,  -3.1892,  -3.2997,  -3.2825,  -3.4276,  -3.0225,  -3.4049,
			                 -3.1225,  -6.2083,  -6.2446,  -6.2835,   5.7589,   1.3265, -17.6486,
			                  1.7717,   1.5729,   5.6839,  -6.2524,  -6.2322,  -6.1767,  -9.8770,
			                  3.3343,  -9.7757,   3.3410,  -3.1535,  -3.2670,  -3.2227,  -3.3343,
			                 -3.3936,  -3.1126,  -3.1875,  -3.2837])
			Loss decreased
			Epoch 0 took 0.55s NLL: 1.4806429148 Reg.: 0.0010508108 Distance: 3.7633790970 Patience: 10
			Loss decreased
			Epoch 1 took 0.70s NLL: 1.4787915945 Reg.: 0.0010553288 Distance: 4.7893419266 Patience: 10
			Loss decreased
			Epoch 2 took 0.76s NLL: 1.4759898186 Reg.: 0.0002319127 Distance: 20.3918857574 Patience: 10
			Loss decreased
			Epoch 3 took 0.70s NLL: 1.4649746418 Reg.: 0.0001459235 Distance: 5.6236004829 Patience: 10
			Loss decreased
			Epoch 4 took 0.61s NLL: 1.4543248415 Reg.: 0.0002109001 Distance: 3.9754855633 Patience: 10
			Loss decreased
			Epoch 5 took 0.64s NLL: 1.4521445036 Reg.: 0.0001490566 Distance: 4.2997198105 Patience: 10
			Loss decreased
			Epoch 6 took 0.62s NLL: 1.4322595596 Reg.: 0.0001581030 Distance: 3.5224432945 Patience: 10
			Loss decreased
			Epoch 7 took 0.53s NLL: 1.4320317507 Reg.: 0.0001949125 Distance: 2.2299807072 Patience: 10
			Loss decreased
			Epoch 8 took 0.11s NLL: 1.4320268631 Reg.: 0.0001983089 Distance: 0.2053759843 Patience: 10
			Loss decreased
			Epoch 9 took 0.09s NLL: 1.4320266247 Reg.: 0.0001983168 Distance: 0.0181633048 Patience: 10
			Loss decreased
			Epoch 10 took 0.62s NLL: 1.4320116043 Reg.: 0.0001600343 Distance: 3.4275653362 Patience: 10
			Loss decreased
			Epoch 11 took 0.60s NLL: 1.4284858704 Reg.: 0.0001545545 Distance: 3.2794635296 Patience: 10
			Loss decreased
			Epoch 12 took 0.60s NLL: 1.3834108114 Reg.: 0.0002172719 Distance: 4.7848982811 Patience: 10
			Loss decreased
			Epoch 13 took 0.56s NLL: 1.3653193712 Reg.: 0.0002516926 Distance: 10.4809799194 Patience: 10
			Epoch 14 took 0.21s NLL: 1.3653193712 Reg.: 0.0002516926 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.489171028137207
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.4835286140441895
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.1600e+00, -8.2326e-01, -1.0868e+00, -2.4318e+00, -1.7752e+00,
			                 6.9695e-01, -3.4925e+00, -9.3125e-01,  1.4204e+00,  1.8172e-01,
			                -7.3195e+00,  2.1537e-01,  6.9452e-03,  2.0810e+00, -1.0602e+00,
			                -6.5295e+00, -8.0163e-01, -3.3235e+00, -4.5224e+00,  3.1454e+00,
			                -1.2789e+00,  2.5078e-02, -4.7174e+00,  4.6926e-01, -1.0986e+00,
			                -1.5093e+00, -1.2188e+00, -1.6753e+00, -1.0223e+00, -1.8142e+00,
			                -1.4857e+00, -1.1797e+00]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.4885566235 Reg.: 0.0016843065 Distance: 0.6163921356 Patience: 10
			Epoch 1 took 0.01s NLL: 1.4885566235 Reg.: 0.0016843065 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.199911117553711
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.45s NLL: 2.2355046272 Reg.: 0.0010595312 Distance: 24.6246643066 Patience: 10
			Epoch 1 took 0.06s NLL: 2.2355046272 Reg.: 0.0010594969 Distance: 0.0006899763 Patience: 9
			Loss decreased
			Epoch 2 took 0.08s NLL: 2.2355046272 Reg.: 0.0010591871 Distance: 0.0057509737 Patience: 10
			Epoch 3 took 0.86s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
			                nan, nan, nan, nan, nan, nan, nan, nan])
			Epoch 0 took 0.63s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -3.2061,  -3.1892,  -3.2997,  -3.2825,  -3.4276,  -3.0225,  -3.4049,
			                 -3.1225,  -6.2083,  -6.2446,  -6.2835,   5.7589,   1.3265, -17.6486,
			                  1.7717,   1.5729,   5.6839,  -6.2524,  -6.2322,  -6.1767,  -9.8770,
			                  3.3343,  -9.7757,   3.3410,  -3.1535,  -3.2670,  -3.2227,  -3.3343,
			                 -3.3936,  -3.1126,  -3.1875,  -3.2837]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.8093485832214355
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.899978160858154
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -3.2061,  -3.1892,  -3.2997,  -3.2825,  -3.4276,  -3.0225,  -3.4049,
			                 -3.1225,  -6.2083,  -6.2446,  -6.2835,   5.7589,   1.3265, -17.6486,
			                  1.7717,   1.5729,   5.6839,  -6.2524,  -6.2322,  -6.1767,  -9.8770,
			                  3.3343,  -9.7757,   3.3410,  -3.1535,  -3.2670,  -3.2227,  -3.3343,
			                 -3.3936,  -3.1126,  -3.1875,  -3.2837])
			Loss decreased
			Epoch 0 took 0.57s NLL: 1.4806429148 Reg.: 0.0010508108 Distance: 3.7633790970 Patience: 10
			Loss decreased
			Epoch 1 took 0.52s NLL: 1.4787915945 Reg.: 0.0010553288 Distance: 4.7893419266 Patience: 10
			Loss decreased
			Epoch 2 took 0.59s NLL: 1.4759898186 Reg.: 0.0002319127 Distance: 20.3918857574 Patience: 10
			Loss decreased
			Epoch 3 took 0.66s NLL: 1.4649746418 Reg.: 0.0001459235 Distance: 5.6236004829 Patience: 10
			Loss decreased
			Epoch 4 took 0.54s NLL: 1.4543248415 Reg.: 0.0002109001 Distance: 3.9754855633 Patience: 10
			Loss decreased
			Epoch 5 took 0.61s NLL: 1.4521445036 Reg.: 0.0001490566 Distance: 4.2997198105 Patience: 10
			Loss decreased
			Epoch 6 took 0.58s NLL: 1.4322595596 Reg.: 0.0001581030 Distance: 3.5224432945 Patience: 10
			Loss decreased
			Epoch 7 took 0.51s NLL: 1.4320317507 Reg.: 0.0001949125 Distance: 2.2299807072 Patience: 10
			Loss decreased
			Epoch 8 took 0.11s NLL: 1.4320268631 Reg.: 0.0001983089 Distance: 0.2053759843 Patience: 10
			Loss decreased
			Epoch 9 took 0.09s NLL: 1.4320266247 Reg.: 0.0001983168 Distance: 0.0181633048 Patience: 10
			Loss decreased
			Epoch 10 took 0.60s NLL: 1.4320116043 Reg.: 0.0001600343 Distance: 3.4275653362 Patience: 10
			Loss decreased
			Epoch 11 took 0.57s NLL: 1.4284858704 Reg.: 0.0001545545 Distance: 3.2794635296 Patience: 10
			Loss decreased
			Epoch 12 took 0.60s NLL: 1.3834108114 Reg.: 0.0002172719 Distance: 4.7848982811 Patience: 10
			Loss decreased
			Epoch 13 took 0.57s NLL: 1.3653193712 Reg.: 0.0002516926 Distance: 10.4809799194 Patience: 10
			Epoch 14 took 0.21s NLL: 1.3653193712 Reg.: 0.0002516926 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.489171028137207
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.4835286140441895
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.1600e+00, -8.2326e-01, -1.0868e+00, -2.4318e+00, -1.7752e+00,
			                 6.9695e-01, -3.4925e+00, -9.3125e-01,  1.4204e+00,  1.8172e-01,
			                -7.3195e+00,  2.1537e-01,  6.9452e-03,  2.0810e+00, -1.0602e+00,
			                -6.5295e+00, -8.0163e-01, -3.3235e+00, -4.5224e+00,  3.1454e+00,
			                -1.2789e+00,  2.5078e-02, -4.7174e+00,  4.6926e-01, -1.0986e+00,
			                -1.5093e+00, -1.2188e+00, -1.6753e+00, -1.0223e+00, -1.8142e+00,
			                -1.4857e+00, -1.1797e+00]) 

