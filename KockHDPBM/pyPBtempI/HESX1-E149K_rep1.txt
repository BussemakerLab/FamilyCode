### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.5837984085 Reg.: 0.0016850556 Distance: 0.7046284676 Patience: 10
			Epoch 1 took 0.01s NLL: 1.5837984085 Reg.: 0.0016850556 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.288147449493408
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.897584915161133
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.988214492797852
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.50s NLL: 1.7768503428 Reg.: 0.0010559805 Distance: 24.6390838623 Patience: 10
			Loss decreased
			Epoch 1 took 0.09s NLL: 1.7768502235 Reg.: 0.0010557992 Distance: 0.0035297456 Patience: 10
			Loss decreased
			Epoch 2 took 0.65s NLL: 1.4851987362 Reg.: 0.0004174029 Distance: 16.1755905151 Patience: 10
			Epoch 3 took 0.19s NLL: 1.4851987362 Reg.: 0.0004174029 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.897584915161133
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.988214492797852
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0786,  -1.3533,  -1.3389,  -1.2993,  -1.3086,  -1.1973,  -1.4579,
			                 -1.1058,  -1.8020,  -1.7648,  -1.9024,   0.3988,   2.6788, -12.4716,
			                  2.3600,   2.3626,   1.1869,  -2.1186,  -1.8707,  -2.2666,  -6.9033,
			                  4.1011,  -6.8829,   4.6148,  -1.3223,  -1.4178,  -1.3549,  -0.9745,
			                 -1.1604,  -1.4113,  -1.0508,  -1.4476]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.897584915161133
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.988214492797852
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0786,  -1.3533,  -1.3389,  -1.2993,  -1.3086,  -1.1973,  -1.4579,
			                 -1.1058,  -1.8020,  -1.7648,  -1.9024,   0.3988,   2.6788, -12.4716,
			                  2.3600,   2.3626,   1.1869,  -2.1186,  -1.8707,  -2.2666,  -6.9033,
			                  4.1011,  -6.8829,   4.6148,  -1.3223,  -1.4178,  -1.3549,  -0.9745,
			                 -1.1604,  -1.4113,  -1.0508,  -1.4476])
			Loss decreased
			Epoch 0 took 0.54s NLL: 1.3486599922 Reg.: 0.0004250805 Distance: 3.1999692917 Patience: 10
			Loss decreased
			Epoch 1 took 0.52s NLL: 1.3481882811 Reg.: 0.0004305627 Distance: 1.2494733334 Patience: 10
			Loss decreased
			Epoch 2 took 0.50s NLL: 1.3482182026 Reg.: 0.0003394688 Distance: 3.0078043938 Patience: 10
			Loss decreased
			Epoch 3 took 0.60s NLL: 1.3477627039 Reg.: 0.0001036479 Distance: 11.5035419464 Patience: 10
			Loss decreased
			Epoch 4 took 0.56s NLL: 1.3469370604 Reg.: 0.0002325535 Distance: 7.7813086510 Patience: 10
			Loss decreased
			Epoch 5 took 0.53s NLL: 1.3469234705 Reg.: 0.0002046076 Distance: 1.4122474194 Patience: 10
			Loss decreased
			Epoch 6 took 0.62s NLL: 1.3468383551 Reg.: 0.0001643155 Distance: 8.0200738907 Patience: 10
			Loss decreased
			Epoch 7 took 0.56s NLL: 1.3466709852 Reg.: 0.0001423056 Distance: 3.6904056072 Patience: 10
			Loss decreased
			Epoch 8 took 0.14s NLL: 1.3466635942 Reg.: 0.0001466367 Distance: 0.4654655755 Patience: 10
			Loss decreased
			Epoch 9 took 0.10s NLL: 1.3466628790 Reg.: 0.0001471947 Distance: 0.0548616238 Patience: 10
			Epoch 10 took 0.22s NLL: 1.3466628790 Reg.: 0.0001471947 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.864259243011475
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -3.9856321811676025
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.9270, -1.0570, -0.7605, -1.2610, -1.0905, -0.8852, -1.1627, -0.8669,
			                -0.7712, -1.1965, -2.5900,  0.5524,  0.2919, -1.9529, -1.0994, -1.2449,
			                 2.9635, -4.4483,  0.6934, -3.2137, -5.0711, -0.5967, -0.7599,  2.4221,
			                -1.1905, -1.7145, -1.2406,  0.1404, -0.3631, -1.6145, -0.2639, -1.7641]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.5837984085 Reg.: 0.0016850556 Distance: 0.7046284676 Patience: 10
			Epoch 1 took 0.01s NLL: 1.5837984085 Reg.: 0.0016850556 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.288147449493408
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.897584915161133
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.988214492797852
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.49s NLL: 1.7768503428 Reg.: 0.0010559805 Distance: 24.6390838623 Patience: 10
			Loss decreased
			Epoch 1 took 0.08s NLL: 1.7768502235 Reg.: 0.0010557992 Distance: 0.0035297456 Patience: 10
			Loss decreased
			Epoch 2 took 0.65s NLL: 1.4851987362 Reg.: 0.0004174029 Distance: 16.1755905151 Patience: 10
			Epoch 3 took 0.18s NLL: 1.4851987362 Reg.: 0.0004174029 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.897584915161133
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.988214492797852
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0786,  -1.3533,  -1.3389,  -1.2993,  -1.3086,  -1.1973,  -1.4579,
			                 -1.1058,  -1.8020,  -1.7648,  -1.9024,   0.3988,   2.6788, -12.4716,
			                  2.3600,   2.3626,   1.1869,  -2.1186,  -1.8707,  -2.2666,  -6.9033,
			                  4.1011,  -6.8829,   4.6148,  -1.3223,  -1.4178,  -1.3549,  -0.9745,
			                 -1.1604,  -1.4113,  -1.0508,  -1.4476]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.897584915161133
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.988214492797852
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0786,  -1.3533,  -1.3389,  -1.2993,  -1.3086,  -1.1973,  -1.4579,
			                 -1.1058,  -1.8020,  -1.7648,  -1.9024,   0.3988,   2.6788, -12.4716,
			                  2.3600,   2.3626,   1.1869,  -2.1186,  -1.8707,  -2.2666,  -6.9033,
			                  4.1011,  -6.8829,   4.6148,  -1.3223,  -1.4178,  -1.3549,  -0.9745,
			                 -1.1604,  -1.4113,  -1.0508,  -1.4476])
			Loss decreased
			Epoch 0 took 0.52s NLL: 1.3486599922 Reg.: 0.0004250805 Distance: 3.1999692917 Patience: 10
			Loss decreased
			Epoch 1 took 0.52s NLL: 1.3481882811 Reg.: 0.0004305627 Distance: 1.2494733334 Patience: 10
			Loss decreased
			Epoch 2 took 0.49s NLL: 1.3482182026 Reg.: 0.0003394688 Distance: 3.0078043938 Patience: 10
			Loss decreased
			Epoch 3 took 0.60s NLL: 1.3477627039 Reg.: 0.0001036479 Distance: 11.5035419464 Patience: 10
			Loss decreased
			Epoch 4 took 0.54s NLL: 1.3469370604 Reg.: 0.0002325535 Distance: 7.7813086510 Patience: 10
			Loss decreased
			Epoch 5 took 0.50s NLL: 1.3469234705 Reg.: 0.0002046076 Distance: 1.4122474194 Patience: 10
			Loss decreased
			Epoch 6 took 0.61s NLL: 1.3468383551 Reg.: 0.0001643155 Distance: 8.0200738907 Patience: 10
			Loss decreased
			Epoch 7 took 0.55s NLL: 1.3466709852 Reg.: 0.0001423056 Distance: 3.6904056072 Patience: 10
			Loss decreased
			Epoch 8 took 0.13s NLL: 1.3466635942 Reg.: 0.0001466367 Distance: 0.4654655755 Patience: 10
			Loss decreased
			Epoch 9 took 0.09s NLL: 1.3466628790 Reg.: 0.0001471947 Distance: 0.0548616238 Patience: 10
			Epoch 10 took 0.21s NLL: 1.3466628790 Reg.: 0.0001471947 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.864259243011475
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -3.9856321811676025
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.9270, -1.0570, -0.7605, -1.2610, -1.0905, -0.8852, -1.1627, -0.8669,
			                -0.7712, -1.1965, -2.5900,  0.5524,  0.2919, -1.9529, -1.0994, -1.2449,
			                 2.9635, -4.4483,  0.6934, -3.2137, -5.0711, -0.5967, -0.7599,  2.4221,
			                -1.1905, -1.7145, -1.2406,  0.1404, -0.3631, -1.6145, -0.2639, -1.7641]) 

