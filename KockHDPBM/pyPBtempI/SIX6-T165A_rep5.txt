### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.7103503942 Reg.: 0.0016849193 Distance: 0.6887326241 Patience: 10
			Epoch 1 took 0.01s NLL: 1.7103503942 Reg.: 0.0016849193 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.272251605987549
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.881689071655273
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.972318649291992
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.42s NLL: 2.0386619568 Reg.: 0.0010564059 Distance: 24.7411499023 Patience: 10
			Loss decreased
			Epoch 1 took 0.55s NLL: 1.5874303579 Reg.: 0.0005545375 Distance: 13.0176572800 Patience: 10
			Epoch 2 took 0.32s NLL: 1.5874303579 Reg.: 0.0005545375 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.881689071655273
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.972318649291992
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.7646,  -1.8111,  -1.7045,  -1.6111,  -1.7316,  -1.7202,  -1.8552,
			                 -1.5841,  -2.1111,  -2.1916,  -2.1864,  -0.4020,   2.1454, -14.2204,
			                  2.8714,   2.3124,   3.5655,  -3.3924,  -3.3503,  -3.7138,  -7.9015,
			                  4.1767,  -7.8299,   4.6637,  -1.2142,  -1.6628,  -2.2409,  -1.7728,
			                 -2.1002,  -1.3336,  -1.8968,  -1.5605]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.881689071655273
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.972318649291992
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.7646,  -1.8111,  -1.7045,  -1.6111,  -1.7316,  -1.7202,  -1.8552,
			                 -1.5841,  -2.1111,  -2.1916,  -2.1864,  -0.4020,   2.1454, -14.2204,
			                  2.8714,   2.3124,   3.5655,  -3.3924,  -3.3503,  -3.7138,  -7.9015,
			                  4.1767,  -7.8299,   4.6637,  -1.2142,  -1.6628,  -2.2409,  -1.7728,
			                 -2.1002,  -1.3336,  -1.8968,  -1.5605])
			Loss decreased
			Epoch 0 took 0.44s NLL: 1.4031567574 Reg.: 0.0005561350 Distance: 2.1209206581 Patience: 10
			Loss decreased
			Epoch 1 took 0.52s NLL: 1.4026386738 Reg.: 0.0004902004 Distance: 2.6504459381 Patience: 10
			Loss decreased
			Epoch 2 took 0.50s NLL: 1.4016853571 Reg.: 0.0004662994 Distance: 1.2560163736 Patience: 10
			Loss decreased
			Epoch 3 took 0.69s NLL: 1.4014539719 Reg.: 0.0004152360 Distance: 7.2121596336 Patience: 10
			Loss decreased
			Epoch 4 took 0.67s NLL: 1.3953713179 Reg.: 0.0007930899 Distance: 19.3706436157 Patience: 10
			Loss decreased
			Epoch 5 took 0.65s NLL: 1.3891172409 Reg.: 0.0009133299 Distance: 3.5101346970 Patience: 10
			Loss decreased
			Epoch 6 took 0.67s NLL: 1.3885979652 Reg.: 0.0009097840 Distance: 0.5702957511 Patience: 10
			Epoch 7 took 0.21s NLL: 1.3885979652 Reg.: 0.0009097840 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.3529791831970215
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.902295112609863
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.1121,  -1.6053,  -1.4605,  -1.1866,  -1.2541,  -1.5998,  -1.3390,
			                 -1.1716,  -1.5386,  -1.3367,  -1.8255,  -0.6637,  -1.4804,  -2.2546,
			                 -0.2969,  -1.3326,   8.6309, -25.3242,   5.7183,   5.6110,  -2.4081,
			                 -0.9392,  -2.3485,   0.3312,   0.1200,  -0.7205,  -2.5300,  -2.2338,
			                 -2.1781,  -0.3937,  -1.7380,  -1.0548]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.7103503942 Reg.: 0.0016849193 Distance: 0.6887326241 Patience: 10
			Epoch 1 took 0.01s NLL: 1.7103503942 Reg.: 0.0016849193 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.272251605987549
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.881689071655273
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.972318649291992
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.43s NLL: 2.0386619568 Reg.: 0.0010564059 Distance: 24.7411499023 Patience: 10
			Loss decreased
			Epoch 1 took 0.58s NLL: 1.5874303579 Reg.: 0.0005545375 Distance: 13.0176572800 Patience: 10
			Epoch 2 took 0.33s NLL: 1.5874303579 Reg.: 0.0005545375 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.881689071655273
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.972318649291992
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.7646,  -1.8111,  -1.7045,  -1.6111,  -1.7316,  -1.7202,  -1.8552,
			                 -1.5841,  -2.1111,  -2.1916,  -2.1864,  -0.4020,   2.1454, -14.2204,
			                  2.8714,   2.3124,   3.5655,  -3.3924,  -3.3503,  -3.7138,  -7.9015,
			                  4.1767,  -7.8299,   4.6637,  -1.2142,  -1.6628,  -2.2409,  -1.7728,
			                 -2.1002,  -1.3336,  -1.8968,  -1.5605]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.881689071655273
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.972318649291992
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.7646,  -1.8111,  -1.7045,  -1.6111,  -1.7316,  -1.7202,  -1.8552,
			                 -1.5841,  -2.1111,  -2.1916,  -2.1864,  -0.4020,   2.1454, -14.2204,
			                  2.8714,   2.3124,   3.5655,  -3.3924,  -3.3503,  -3.7138,  -7.9015,
			                  4.1767,  -7.8299,   4.6637,  -1.2142,  -1.6628,  -2.2409,  -1.7728,
			                 -2.1002,  -1.3336,  -1.8968,  -1.5605])
			Loss decreased
			Epoch 0 took 0.44s NLL: 1.4031567574 Reg.: 0.0005561350 Distance: 2.1209206581 Patience: 10
			Loss decreased
			Epoch 1 took 0.54s NLL: 1.4026386738 Reg.: 0.0004902004 Distance: 2.6504459381 Patience: 10
			Loss decreased
			Epoch 2 took 0.48s NLL: 1.4016853571 Reg.: 0.0004662994 Distance: 1.2560163736 Patience: 10
			Loss decreased
			Epoch 3 took 0.49s NLL: 1.4014539719 Reg.: 0.0004152360 Distance: 7.2121596336 Patience: 10
			Loss decreased
			Epoch 4 took 0.57s NLL: 1.3953713179 Reg.: 0.0007930899 Distance: 19.3706436157 Patience: 10
			Loss decreased
			Epoch 5 took 0.56s NLL: 1.3891172409 Reg.: 0.0009133299 Distance: 3.5101346970 Patience: 10
			Loss decreased
			Epoch 6 took 0.68s NLL: 1.3885979652 Reg.: 0.0009097840 Distance: 0.5702957511 Patience: 10
			Epoch 7 took 0.21s NLL: 1.3885979652 Reg.: 0.0009097840 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.3529791831970215
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.902295112609863
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.1121,  -1.6053,  -1.4605,  -1.1866,  -1.2541,  -1.5998,  -1.3390,
			                 -1.1716,  -1.5386,  -1.3367,  -1.8255,  -0.6637,  -1.4804,  -2.2546,
			                 -0.2969,  -1.3326,   8.6309, -25.3242,   5.7183,   5.6110,  -2.4081,
			                 -0.9392,  -2.3485,   0.3312,   0.1200,  -0.7205,  -2.5300,  -2.2338,
			                 -2.1781,  -0.3937,  -1.7380,  -1.0548]) 

