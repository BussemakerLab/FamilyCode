### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.7288595438 Reg.: 0.0016845803 Distance: 0.6488599777 Patience: 10
			Epoch 1 took 0.01s NLL: 1.7288595438 Reg.: 0.0016845803 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.232378959655762
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.39s NLL: 2.0390124321 Reg.: 0.0010556635 Distance: 24.7804889679 Patience: 10
			Epoch 1 took 0.05s NLL: 2.0390124321 Reg.: 0.0010556267 Distance: 0.0009129397 Patience: 9
			Loss decreased
			Epoch 2 took 0.56s NLL: 1.4068315029 Reg.: 0.0005810075 Distance: 12.9130382538 Patience: 10
			Loss decreased
			Epoch 3 took 0.46s NLL: 1.3279396296 Reg.: 0.0005797268 Distance: 1.4529438019 Patience: 10
			Epoch 4 took 0.74s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
			                nan, nan, nan, nan, nan, nan, nan, nan])
			Epoch 0 took 0.56s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.6623,  -2.1444,  -1.9669,  -1.6720,  -1.7576,  -2.0551,  -1.9089,
			                 -1.7240,  -2.0044,  -1.8590,  -2.3060,  -1.2760,   1.8916, -14.4616,
			                  3.0207,   2.1034,   3.4781,  -3.5214,  -3.4844,  -3.9177,  -8.0680,
			                  3.5096,  -7.9880,   5.1011,  -0.5191,  -1.2178,  -2.7871,  -2.9215,
			                 -2.6125,  -1.0200,  -2.2330,  -1.5799]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.6623,  -2.1444,  -1.9669,  -1.6720,  -1.7576,  -2.0551,  -1.9089,
			                 -1.7240,  -2.0044,  -1.8590,  -2.3060,  -1.2760,   1.8916, -14.4616,
			                  3.0207,   2.1034,   3.4781,  -3.5214,  -3.4844,  -3.9177,  -8.0680,
			                  3.5096,  -7.9880,   5.1011,  -0.5191,  -1.2178,  -2.7871,  -2.9215,
			                 -2.6125,  -1.0200,  -2.2330,  -1.5799])
			Loss decreased
			Epoch 0 took 0.52s NLL: 1.3275464773 Reg.: 0.0005186656 Distance: 2.4912064075 Patience: 10
			Loss decreased
			Epoch 1 took 0.52s NLL: 1.3252062798 Reg.: 0.0004945302 Distance: 1.2861675024 Patience: 10
			Loss decreased
			Epoch 2 took 0.40s NLL: 1.3250355721 Reg.: 0.0004849712 Distance: 0.6584427357 Patience: 10
			Loss decreased
			Epoch 3 took 0.08s NLL: 1.3250354528 Reg.: 0.0004849040 Distance: 0.0100040920 Patience: 10
			Loss decreased
			Epoch 4 took 0.53s NLL: 1.3203352690 Reg.: 0.0001045492 Distance: 14.7721176147 Patience: 10
			Loss decreased
			Epoch 5 took 0.51s NLL: 1.3103539944 Reg.: 0.0001638956 Distance: 5.2886834145 Patience: 10
			Loss decreased
			Epoch 6 took 0.33s NLL: 1.3103650808 Reg.: 0.0001348048 Distance: 1.4331709146 Patience: 10
			Loss decreased
			Epoch 7 took 0.24s NLL: 1.3103642464 Reg.: 0.0001354339 Distance: 0.0330842622 Patience: 10
			Epoch 8 took 0.20s NLL: 1.3103642464 Reg.: 0.0001354339 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -9.57078742980957
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -2.3728466033935547
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.4828, -0.9678, -0.7835, -0.5536, -0.6020, -0.8897, -0.7580, -0.5380,
			                -0.9210, -0.6864, -1.0982, -0.0817, -0.7835, -1.7121,  0.3302, -0.6218,
			                 1.6266, -1.8237, -1.1587, -1.4313, -1.8359, -0.5438, -1.5362,  1.1290,
			                 0.7619,  0.0878, -1.9846, -1.6531, -1.4941,  0.1825, -1.0870, -0.3896]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.7288595438 Reg.: 0.0016845803 Distance: 0.6488599777 Patience: 10
			Epoch 1 took 0.01s NLL: 1.7288595438 Reg.: 0.0016845803 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.232378959655762
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.40s NLL: 2.0390124321 Reg.: 0.0010556635 Distance: 24.7804889679 Patience: 10
			Epoch 1 took 0.05s NLL: 2.0390124321 Reg.: 0.0010556267 Distance: 0.0009129397 Patience: 9
			Loss decreased
			Epoch 2 took 0.58s NLL: 1.4068315029 Reg.: 0.0005810075 Distance: 12.9130382538 Patience: 10
			Loss decreased
			Epoch 3 took 0.44s NLL: 1.3279396296 Reg.: 0.0005797268 Distance: 1.4529438019 Patience: 10
			Epoch 4 took 0.75s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
			                nan, nan, nan, nan, nan, nan, nan, nan])
			Epoch 0 took 0.57s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.6623,  -2.1444,  -1.9669,  -1.6720,  -1.7576,  -2.0551,  -1.9089,
			                 -1.7240,  -2.0044,  -1.8590,  -2.3060,  -1.2760,   1.8916, -14.4616,
			                  3.0207,   2.1034,   3.4781,  -3.5214,  -3.4844,  -3.9177,  -8.0680,
			                  3.5096,  -7.9880,   5.1011,  -0.5191,  -1.2178,  -2.7871,  -2.9215,
			                 -2.6125,  -1.0200,  -2.2330,  -1.5799]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.841816425323486
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.932446002960205
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.6623,  -2.1444,  -1.9669,  -1.6720,  -1.7576,  -2.0551,  -1.9089,
			                 -1.7240,  -2.0044,  -1.8590,  -2.3060,  -1.2760,   1.8916, -14.4616,
			                  3.0207,   2.1034,   3.4781,  -3.5214,  -3.4844,  -3.9177,  -8.0680,
			                  3.5096,  -7.9880,   5.1011,  -0.5191,  -1.2178,  -2.7871,  -2.9215,
			                 -2.6125,  -1.0200,  -2.2330,  -1.5799])
			Loss decreased
			Epoch 0 took 0.52s NLL: 1.3275464773 Reg.: 0.0005186656 Distance: 2.4912064075 Patience: 10
			Loss decreased
			Epoch 1 took 0.51s NLL: 1.3252062798 Reg.: 0.0004945302 Distance: 1.2861675024 Patience: 10
			Loss decreased
			Epoch 2 took 0.42s NLL: 1.3250355721 Reg.: 0.0004849712 Distance: 0.6584427357 Patience: 10
			Loss decreased
			Epoch 3 took 0.08s NLL: 1.3250354528 Reg.: 0.0004849040 Distance: 0.0100040920 Patience: 10
			Loss decreased
			Epoch 4 took 0.54s NLL: 1.3203352690 Reg.: 0.0001045492 Distance: 14.7721176147 Patience: 10
			Loss decreased
			Epoch 5 took 0.49s NLL: 1.3103539944 Reg.: 0.0001638956 Distance: 5.2886834145 Patience: 10
			Loss decreased
			Epoch 6 took 0.34s NLL: 1.3103650808 Reg.: 0.0001348048 Distance: 1.4331709146 Patience: 10
			Loss decreased
			Epoch 7 took 0.26s NLL: 1.3103642464 Reg.: 0.0001354339 Distance: 0.0330842622 Patience: 10
			Epoch 8 took 0.23s NLL: 1.3103642464 Reg.: 0.0001354339 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -9.57078742980957
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -2.3728466033935547
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.4828, -0.9678, -0.7835, -0.5536, -0.6020, -0.8897, -0.7580, -0.5380,
			                -0.9210, -0.6864, -1.0982, -0.0817, -0.7835, -1.7121,  0.3302, -0.6218,
			                 1.6266, -1.8237, -1.1587, -1.4313, -1.8359, -0.5438, -1.5362,  1.1290,
			                 0.7619,  0.0878, -1.9846, -1.6531, -1.4941,  0.1825, -1.0870, -0.3896]) 

