### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.6638609171 Reg.: 0.0016850246 Distance: 0.7010250092 Patience: 10
			Epoch 1 took 0.01s NLL: 1.6638609171 Reg.: 0.0016850246 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.284543991088867
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.893981456756592
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.9846110343933105
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.42s NLL: 1.3387731314 Reg.: 0.0010498498 Distance: 24.8382759094 Patience: 10
			Loss decreased
			Epoch 1 took 0.08s NLL: 1.3387730122 Reg.: 0.0010498277 Distance: 0.0008575270 Patience: 10
			Loss decreased
			Epoch 2 took 0.74s NLL: 1.2296538353 Reg.: 0.0005105750 Distance: 12.5879983902 Patience: 10
			Loss decreased
			Epoch 3 took 0.49s NLL: 1.1825226545 Reg.: 0.0004844617 Distance: 1.2656337023 Patience: 10
			Loss decreased
			Epoch 4 took 0.54s NLL: 1.1819980145 Reg.: 0.0004536551 Distance: 1.9421646595 Patience: 10
			Loss decreased
			Epoch 5 took 0.25s NLL: 1.1818509102 Reg.: 0.0004742891 Distance: 2.7900567055 Patience: 10
			Loss decreased
			Epoch 6 took 0.51s NLL: 1.1810714006 Reg.: 0.0001619920 Distance: 11.9460096359 Patience: 10
			Loss decreased
			Epoch 7 took 0.56s NLL: 1.1770032644 Reg.: 0.0001925550 Distance: 4.6594901085 Patience: 10
			Loss decreased
			Epoch 8 took 0.56s NLL: 1.1768850088 Reg.: 0.0001609243 Distance: 3.1630294323 Patience: 10
			Epoch 9 took 0.05s NLL: 1.1768850088 Reg.: 0.0001609413 Distance: 0.0103216609 Patience: 9
			Loss decreased
			Epoch 10 took 0.08s NLL: 1.1768848896 Reg.: 0.0001609611 Distance: 0.0047605420 Patience: 10
			Loss decreased
			Epoch 11 took 0.28s NLL: 1.1768847704 Reg.: 0.0001609665 Distance: 0.0003709828 Patience: 10
			Epoch 12 took 0.17s NLL: 1.1768847704 Reg.: 0.0001609665 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.893981456756592
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.9846110343933105
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.9791, -0.8974, -0.8920, -0.8681, -1.0583, -0.8528, -1.1453, -0.5803,
			                -0.4420, -2.2370, -1.9615,  1.0018,  0.7158, -1.9111, -1.1539, -1.2876,
			                 4.4578, -4.1718, -0.0642, -3.8564, -3.1324, -0.0865, -2.2679,  1.8503,
			                -0.8054, -1.0837, -1.1468, -0.6008, -0.9402, -0.7898, -0.9994, -0.9075]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.893981456756592
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.9846110343933105
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.9791, -0.8974, -0.8920, -0.8681, -1.0583, -0.8528, -1.1453, -0.5803,
			                -0.4420, -2.2370, -1.9615,  1.0018,  0.7158, -1.9111, -1.1539, -1.2876,
			                 4.4578, -4.1718, -0.0642, -3.8564, -3.1324, -0.0865, -2.2679,  1.8503,
			                -0.8054, -1.0837, -1.1468, -0.6008, -0.9402, -0.7898, -0.9994, -0.9075])
			Loss decreased
			Epoch 0 took 0.44s NLL: 1.1730142832 Reg.: 0.0001635191 Distance: 0.9663153291 Patience: 10
			Loss decreased
			Epoch 1 took 0.44s NLL: 1.1718041897 Reg.: 0.0001586800 Distance: 1.2321681976 Patience: 10
			Loss decreased
			Epoch 2 took 0.41s NLL: 1.1717667580 Reg.: 0.0001587956 Distance: 0.1005126387 Patience: 10
			Epoch 3 took 0.22s NLL: 1.1717667580 Reg.: 0.0001587956 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -7.138973712921143
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.543957710266113
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.8827, -0.7763, -0.7767, -0.7647, -0.9669, -0.7351, -1.0145, -0.4840,
			                -0.4966, -1.8594, -1.6636,  0.8172,  0.6680, -1.6549, -1.0622, -1.1514,
			                 3.9900, -4.1571,  0.8277, -3.8590, -3.0450, -0.0901, -1.7266,  1.6615,
			                -0.6806, -0.9642, -1.0078, -0.5479, -0.8528, -0.6692, -0.8788, -0.7999]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.6638609171 Reg.: 0.0016850246 Distance: 0.7010250092 Patience: 10
			Epoch 1 took 0.01s NLL: 1.6638609171 Reg.: 0.0016850246 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.284543991088867
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.893981456756592
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.9846110343933105
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.48s NLL: 1.3387731314 Reg.: 0.0010498498 Distance: 24.8382759094 Patience: 10
			Loss decreased
			Epoch 1 took 0.08s NLL: 1.3387730122 Reg.: 0.0010498277 Distance: 0.0008575270 Patience: 10
			Loss decreased
			Epoch 2 took 0.78s NLL: 1.2296538353 Reg.: 0.0005105750 Distance: 12.5879983902 Patience: 10
			Loss decreased
			Epoch 3 took 0.53s NLL: 1.1825226545 Reg.: 0.0004844617 Distance: 1.2656337023 Patience: 10
			Loss decreased
			Epoch 4 took 0.56s NLL: 1.1819980145 Reg.: 0.0004536551 Distance: 1.9421646595 Patience: 10
			Loss decreased
			Epoch 5 took 0.25s NLL: 1.1818509102 Reg.: 0.0004742891 Distance: 2.7900567055 Patience: 10
			Loss decreased
			Epoch 6 took 0.55s NLL: 1.1810714006 Reg.: 0.0001619920 Distance: 11.9460096359 Patience: 10
			Loss decreased
			Epoch 7 took 0.53s NLL: 1.1770032644 Reg.: 0.0001925550 Distance: 4.6594901085 Patience: 10
			Loss decreased
			Epoch 8 took 0.55s NLL: 1.1768850088 Reg.: 0.0001609243 Distance: 3.1630294323 Patience: 10
			Epoch 9 took 0.05s NLL: 1.1768850088 Reg.: 0.0001609413 Distance: 0.0103216609 Patience: 9
			Loss decreased
			Epoch 10 took 0.08s NLL: 1.1768848896 Reg.: 0.0001609611 Distance: 0.0047605420 Patience: 10
			Loss decreased
			Epoch 11 took 0.29s NLL: 1.1768847704 Reg.: 0.0001609665 Distance: 0.0003709828 Patience: 10
			Epoch 12 took 0.19s NLL: 1.1768847704 Reg.: 0.0001609665 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.893981456756592
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.9846110343933105
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.9791, -0.8974, -0.8920, -0.8681, -1.0583, -0.8528, -1.1453, -0.5803,
			                -0.4420, -2.2370, -1.9615,  1.0018,  0.7158, -1.9111, -1.1539, -1.2876,
			                 4.4578, -4.1718, -0.0642, -3.8564, -3.1324, -0.0865, -2.2679,  1.8503,
			                -0.8054, -1.0837, -1.1468, -0.6008, -0.9402, -0.7898, -0.9994, -0.9075]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.893981456756592
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.9846110343933105
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.9791, -0.8974, -0.8920, -0.8681, -1.0583, -0.8528, -1.1453, -0.5803,
			                -0.4420, -2.2370, -1.9615,  1.0018,  0.7158, -1.9111, -1.1539, -1.2876,
			                 4.4578, -4.1718, -0.0642, -3.8564, -3.1324, -0.0865, -2.2679,  1.8503,
			                -0.8054, -1.0837, -1.1468, -0.6008, -0.9402, -0.7898, -0.9994, -0.9075])
			Loss decreased
			Epoch 0 took 0.47s NLL: 1.1730142832 Reg.: 0.0001635191 Distance: 0.9663153291 Patience: 10
			Loss decreased
			Epoch 1 took 0.47s NLL: 1.1718041897 Reg.: 0.0001586800 Distance: 1.2321681976 Patience: 10
			Loss decreased
			Epoch 2 took 0.41s NLL: 1.1717667580 Reg.: 0.0001587956 Distance: 0.1005126387 Patience: 10
			Epoch 3 took 0.21s NLL: 1.1717667580 Reg.: 0.0001587956 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -7.138973712921143
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.543957710266113
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.8827, -0.7763, -0.7767, -0.7647, -0.9669, -0.7351, -1.0145, -0.4840,
			                -0.4966, -1.8594, -1.6636,  0.8172,  0.6680, -1.6549, -1.0622, -1.1514,
			                 3.9900, -4.1571,  0.8277, -3.8590, -3.0450, -0.0901, -1.7266,  1.6615,
			                -0.6806, -0.9642, -1.0078, -0.5479, -0.8528, -0.6692, -0.8788, -0.7999]) 

