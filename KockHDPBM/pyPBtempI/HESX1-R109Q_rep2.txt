### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 3.2829468250 Reg.: 0.0017036297 Distance: 2.4961576462 Patience: 10
			Epoch 1 took 0.01s NLL: 3.2829468250 Reg.: 0.0017036297 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.079676628112793
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -7.689114570617676
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -6.779743671417236
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.29s NLL: 2.6409344673 Reg.: 0.0011088495 Distance: 25.5465812683 Patience: 10
			Loss decreased
			Epoch 1 took 0.05s NLL: 2.6409337521 Reg.: 0.0011086860 Distance: 0.0062771859 Patience: 10
			Loss decreased
			Epoch 2 took 0.28s NLL: 2.4528779984 Reg.: 0.0007848440 Distance: 9.4178009033 Patience: 10
			Epoch 3 took 0.09s NLL: 2.4528779984 Reg.: 0.0007848440 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -7.689114570617676
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -6.779743671417236
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -2.2708,  -2.3428,  -2.1161,  -2.4850,  -2.7348,  -1.9886,  -2.6484,
			                 -1.8431,  -2.4542,  -2.2622,  -4.1622,  -0.3364,   3.3517, -15.8344,
			                  2.1620,   1.1060,   4.3049,  -4.6218,  -4.7757,  -4.1225,  -9.1944,
			                  3.5163,  -9.2111,   5.6743,  -2.7217,  -2.8862,  -2.6394,  -0.9678,
			                 -1.7456,  -2.8814,  -1.5789,  -3.0091]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -7.689114570617676
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -6.779743671417236
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -2.2708,  -2.3428,  -2.1161,  -2.4850,  -2.7348,  -1.9886,  -2.6484,
			                 -1.8431,  -2.4542,  -2.2622,  -4.1622,  -0.3364,   3.3517, -15.8344,
			                  2.1620,   1.1060,   4.3049,  -4.6218,  -4.7757,  -4.1225,  -9.1944,
			                  3.5163,  -9.2111,   5.6743,  -2.7217,  -2.8862,  -2.6394,  -0.9678,
			                 -1.7456,  -2.8814,  -1.5789,  -3.0091])
			Loss decreased
			Epoch 0 took 0.28s NLL: 2.2559757233 Reg.: 0.0008199289 Distance: 4.4179854393 Patience: 10
			Loss decreased
			Epoch 1 took 0.27s NLL: 2.2489888668 Reg.: 0.0008280432 Distance: 1.3901630640 Patience: 10
			Loss decreased
			Epoch 2 took 0.31s NLL: 2.2487578392 Reg.: 0.0008144929 Distance: 3.4415545464 Patience: 10
			Loss decreased
			Epoch 3 took 0.14s NLL: 2.2486801147 Reg.: 0.0008081368 Distance: 0.2862901688 Patience: 10
			Loss decreased
			Epoch 4 took 0.04s NLL: 2.2486803532 Reg.: 0.0008078458 Distance: 0.0102252802 Patience: 10
			Loss decreased
			Epoch 5 took 0.20s NLL: 2.2486112118 Reg.: 0.0006128064 Distance: 10.0196876526 Patience: 10
			Loss decreased
			Epoch 6 took 0.32s NLL: 2.2453653812 Reg.: 0.0008083498 Distance: 14.7603778839 Patience: 10
			Loss decreased
			Epoch 7 took 0.33s NLL: 2.2444546223 Reg.: 0.0007159058 Distance: 7.3302655220 Patience: 10
			Loss decreased
			Epoch 8 took 0.34s NLL: 2.2442553043 Reg.: 0.0005764134 Distance: 10.7657566071 Patience: 10
			Loss decreased
			Epoch 9 took 0.07s NLL: 2.2442557812 Reg.: 0.0005750240 Distance: 0.1611420959 Patience: 10
			Loss decreased
			Epoch 10 took 0.90s NLL: 2.2442526817 Reg.: 0.0005403064 Distance: 5.0896315575 Patience: 10
			Loss decreased
			Epoch 11 took 0.35s NLL: 2.2442810535 Reg.: 0.0003305464 Distance: 7.1826920509 Patience: 10
			Loss decreased
			Epoch 12 took 0.25s NLL: 2.2442808151 Reg.: 0.0003305453 Distance: 0.0002402871 Patience: 10
			Epoch 13 took 0.10s NLL: 2.2442808151 Reg.: 0.0003305453 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.9144158363342285
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -6.256661891937256
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.2868, -2.0955, -1.6597, -2.0429, -2.7128, -1.0048, -2.3079, -1.0598,
			                -2.8543,  0.3164, -4.9884,  0.4410,  0.9757, -2.7265, -3.1320, -2.2026,
			                 4.4636, -5.7655, -2.7119, -3.0714, -1.4213, -2.4489, -6.0861,  2.8713,
			                -2.4900, -1.6284, -2.9960,  0.0294,  0.2202, -2.9807, -2.3300, -1.9947]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 3.2829468250 Reg.: 0.0017036297 Distance: 2.4961576462 Patience: 10
			Epoch 1 took 0.01s NLL: 3.2829468250 Reg.: 0.0017036297 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.079676628112793
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -7.689114570617676
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -6.779743671417236
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.25s NLL: 2.6409344673 Reg.: 0.0011088495 Distance: 25.5465812683 Patience: 10
			Loss decreased
			Epoch 1 took 0.04s NLL: 2.6409337521 Reg.: 0.0011086860 Distance: 0.0062771859 Patience: 10
			Loss decreased
			Epoch 2 took 0.25s NLL: 2.4528779984 Reg.: 0.0007848440 Distance: 9.4178009033 Patience: 10
			Epoch 3 took 0.10s NLL: 2.4528779984 Reg.: 0.0007848440 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -7.689114570617676
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -6.779743671417236
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -2.2708,  -2.3428,  -2.1161,  -2.4850,  -2.7348,  -1.9886,  -2.6484,
			                 -1.8431,  -2.4542,  -2.2622,  -4.1622,  -0.3364,   3.3517, -15.8344,
			                  2.1620,   1.1060,   4.3049,  -4.6218,  -4.7757,  -4.1225,  -9.1944,
			                  3.5163,  -9.2111,   5.6743,  -2.7217,  -2.8862,  -2.6394,  -0.9678,
			                 -1.7456,  -2.8814,  -1.5789,  -3.0091]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -7.689114570617676
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -6.779743671417236
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -2.2708,  -2.3428,  -2.1161,  -2.4850,  -2.7348,  -1.9886,  -2.6484,
			                 -1.8431,  -2.4542,  -2.2622,  -4.1622,  -0.3364,   3.3517, -15.8344,
			                  2.1620,   1.1060,   4.3049,  -4.6218,  -4.7757,  -4.1225,  -9.1944,
			                  3.5163,  -9.2111,   5.6743,  -2.7217,  -2.8862,  -2.6394,  -0.9678,
			                 -1.7456,  -2.8814,  -1.5789,  -3.0091])
			Loss decreased
			Epoch 0 took 0.30s NLL: 2.2559757233 Reg.: 0.0008199289 Distance: 4.4179854393 Patience: 10
			Loss decreased
			Epoch 1 took 0.25s NLL: 2.2489888668 Reg.: 0.0008280432 Distance: 1.3901630640 Patience: 10
			Loss decreased
			Epoch 2 took 0.28s NLL: 2.2487578392 Reg.: 0.0008144929 Distance: 3.4415545464 Patience: 10
			Loss decreased
			Epoch 3 took 0.13s NLL: 2.2486801147 Reg.: 0.0008081368 Distance: 0.2862901688 Patience: 10
			Loss decreased
			Epoch 4 took 0.05s NLL: 2.2486803532 Reg.: 0.0008078458 Distance: 0.0102252802 Patience: 10
			Loss decreased
			Epoch 5 took 0.20s NLL: 2.2486112118 Reg.: 0.0006128064 Distance: 10.0196876526 Patience: 10
			Loss decreased
			Epoch 6 took 0.30s NLL: 2.2453653812 Reg.: 0.0008083498 Distance: 14.7603778839 Patience: 10
			Loss decreased
			Epoch 7 took 0.31s NLL: 2.2444546223 Reg.: 0.0007159058 Distance: 7.3302655220 Patience: 10
			Loss decreased
			Epoch 8 took 0.29s NLL: 2.2442553043 Reg.: 0.0005764134 Distance: 10.7657566071 Patience: 10
			Loss decreased
			Epoch 9 took 0.06s NLL: 2.2442557812 Reg.: 0.0005750240 Distance: 0.1611420959 Patience: 10
			Loss decreased
			Epoch 10 took 0.30s NLL: 2.2442526817 Reg.: 0.0005403064 Distance: 5.0896315575 Patience: 10
			Loss decreased
			Epoch 11 took 0.22s NLL: 2.2442810535 Reg.: 0.0003305464 Distance: 7.1826920509 Patience: 10
			Loss decreased
			Epoch 12 took 0.17s NLL: 2.2442808151 Reg.: 0.0003305453 Distance: 0.0002402871 Patience: 10
			Epoch 13 took 0.08s NLL: 2.2442808151 Reg.: 0.0003305453 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.9144158363342285
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -6.256661891937256
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.2868, -2.0955, -1.6597, -2.0429, -2.7128, -1.0048, -2.3079, -1.0598,
			                -2.8543,  0.3164, -4.9884,  0.4410,  0.9757, -2.7265, -3.1320, -2.2026,
			                 4.4636, -5.7655, -2.7119, -3.0714, -1.4213, -2.4489, -6.0861,  2.8713,
			                -2.4900, -1.6284, -2.9960,  0.0294,  0.2202, -2.9807, -2.3300, -1.9947]) 

