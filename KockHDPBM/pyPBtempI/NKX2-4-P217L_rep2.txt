### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.5295701027 Reg.: 0.0016837009 Distance: 0.5436687469 Patience: 10
			Epoch 1 took 0.01s NLL: 1.5295701027 Reg.: 0.0016837009 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.127187728881836
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.41s NLL: 2.2784717083 Reg.: 0.0010579850 Distance: 24.6394367218 Patience: 10
			Loss decreased
			Epoch 1 took 0.08s NLL: 2.2784714699 Reg.: 0.0010579251 Distance: 0.0015099421 Patience: 10
			Epoch 2 took 0.05s NLL: 2.2784714699 Reg.: 0.0010578322 Distance: 0.0018000653 Patience: 9
			Loss decreased
			Epoch 3 took 0.61s NLL: 1.7281570435 Reg.: 0.0003658189 Distance: 17.7583637238 Patience: 10
			Loss decreased
			Epoch 4 took 0.52s NLL: 1.4145686626 Reg.: 0.0003175421 Distance: 4.0331563950 Patience: 10
			Loss decreased
			Epoch 5 took 0.49s NLL: 1.3537127972 Reg.: 0.0003367682 Distance: 2.2999842167 Patience: 10
			Loss decreased
			Epoch 6 took 0.22s NLL: 1.3537087440 Reg.: 0.0003356270 Distance: 0.0662351176 Patience: 10
			Loss decreased
			Epoch 7 took 0.08s NLL: 1.3537087440 Reg.: 0.0003354796 Distance: 0.0052729775 Patience: 10
			Epoch 8 took 1.17s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
			                nan, nan, nan, nan, nan, nan, nan, nan])
			Epoch 0 took 0.68s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0697,  -1.3225,  -1.0758,  -1.1682,  -1.1296,  -0.8728,  -0.9197,
			                 -1.7186,  -1.7438,  -0.1471,  -2.0937,  -0.6307,   3.3513, -11.4026,
			                  0.8456,   2.5693,  -1.2576,   0.4669,  -2.1872,  -1.6497,  -5.8298,
			                  2.5055,  -5.4377,   4.1202,  -1.6122,  -0.3648,  -2.6822,   0.0319,
			                 -0.8301,  -1.3508,  -1.1312,  -1.3306]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0697,  -1.3225,  -1.0758,  -1.1682,  -1.1296,  -0.8728,  -0.9197,
			                 -1.7186,  -1.7438,  -0.1471,  -2.0937,  -0.6307,   3.3513, -11.4026,
			                  0.8456,   2.5693,  -1.2576,   0.4669,  -2.1872,  -1.6497,  -5.8298,
			                  2.5055,  -5.4377,   4.1202,  -1.6122,  -0.3648,  -2.6822,   0.0319,
			                 -0.8301,  -1.3508,  -1.1312,  -1.3306])
			Loss decreased
			Epoch 0 took 0.50s NLL: 1.3241962194 Reg.: 0.0003780853 Distance: 4.0392937660 Patience: 10
			Loss decreased
			Epoch 1 took 0.50s NLL: 1.3222550154 Reg.: 0.0004260489 Distance: 3.2133171558 Patience: 10
			Loss decreased
			Epoch 2 took 0.51s NLL: 1.3219883442 Reg.: 0.0004477386 Distance: 2.6848759651 Patience: 10
			Loss decreased
			Epoch 3 took 0.63s NLL: 1.3213230371 Reg.: 0.0002602257 Distance: 12.9209585190 Patience: 10
			Loss decreased
			Epoch 4 took 0.60s NLL: 1.2876080275 Reg.: 0.0002969088 Distance: 3.6854357719 Patience: 10
			Loss decreased
			Epoch 5 took 0.55s NLL: 1.2729550600 Reg.: 0.0002997268 Distance: 1.9017144442 Patience: 10
			Loss decreased
			Epoch 6 took 0.50s NLL: 1.2728049755 Reg.: 0.0001724742 Distance: 5.7631349564 Patience: 10
			Epoch 7 took 0.05s NLL: 1.2728050947 Reg.: 0.0001723609 Distance: 0.0093386229 Patience: 9
			Loss decreased
			Epoch 8 took 0.58s NLL: 1.2723785639 Reg.: 0.0001794007 Distance: 4.1254005432 Patience: 10
			Loss decreased
			Epoch 9 took 0.19s NLL: 1.2723627090 Reg.: 0.0001834576 Distance: 0.6995053291 Patience: 10
			Loss decreased
			Epoch 10 took 0.21s NLL: 1.2723625898 Reg.: 0.0001835115 Distance: 0.0315955430 Patience: 10
			Epoch 11 took 0.16s NLL: 1.2723625898 Reg.: 0.0001835115 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.502890586853027
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.10325813293457
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.8806, -1.2694, -0.9004, -1.0195, -0.8920, -0.7084, -0.7085, -1.7620,
			                -1.4091,  0.4081, -2.9851, -0.0807,  0.4065, -0.5954, -3.3992, -0.4816,
			                -0.7030,  1.1139, -2.2899, -2.1894,  0.3042, -2.9861, -4.5007,  3.1119,
			                -0.2912,  1.1930, -6.6605,  1.6901, -0.6291, -1.2893, -0.9181, -1.2355]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.5295701027 Reg.: 0.0016837009 Distance: 0.5436687469 Patience: 10
			Epoch 1 took 0.01s NLL: 1.5295701027 Reg.: 0.0016837009 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -4.127187728881836
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.41s NLL: 2.2784717083 Reg.: 0.0010579850 Distance: 24.6394367218 Patience: 10
			Loss decreased
			Epoch 1 took 0.08s NLL: 2.2784714699 Reg.: 0.0010579251 Distance: 0.0015099421 Patience: 10
			Epoch 2 took 0.06s NLL: 2.2784714699 Reg.: 0.0010578322 Distance: 0.0018000653 Patience: 9
			Loss decreased
			Epoch 3 took 0.64s NLL: 1.7281570435 Reg.: 0.0003658189 Distance: 17.7583637238 Patience: 10
			Loss decreased
			Epoch 4 took 0.53s NLL: 1.4145686626 Reg.: 0.0003175421 Distance: 4.0331563950 Patience: 10
			Loss decreased
			Epoch 5 took 0.46s NLL: 1.3537127972 Reg.: 0.0003367682 Distance: 2.2999842167 Patience: 10
			Loss decreased
			Epoch 6 took 0.23s NLL: 1.3537087440 Reg.: 0.0003356270 Distance: 0.0662351176 Patience: 10
			Loss decreased
			Epoch 7 took 0.09s NLL: 1.3537087440 Reg.: 0.0003354796 Distance: 0.0052729775 Patience: 10
			Epoch 8 took 0.90s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
			                nan, nan, nan, nan, nan, nan, nan, nan])
			Epoch 0 took 0.58s NLL: nan Reg.: nan Distance: nan Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0697,  -1.3225,  -1.0758,  -1.1682,  -1.1296,  -0.8728,  -0.9197,
			                 -1.7186,  -1.7438,  -0.1471,  -2.0937,  -0.6307,   3.3513, -11.4026,
			                  0.8456,   2.5693,  -1.2576,   0.4669,  -2.1872,  -1.6497,  -5.8298,
			                  2.5055,  -5.4377,   4.1202,  -1.6122,  -0.3648,  -2.6822,   0.0319,
			                 -0.8301,  -1.3508,  -1.1312,  -1.3306]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.7366251945495605
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.827254772186279
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ -1.0697,  -1.3225,  -1.0758,  -1.1682,  -1.1296,  -0.8728,  -0.9197,
			                 -1.7186,  -1.7438,  -0.1471,  -2.0937,  -0.6307,   3.3513, -11.4026,
			                  0.8456,   2.5693,  -1.2576,   0.4669,  -2.1872,  -1.6497,  -5.8298,
			                  2.5055,  -5.4377,   4.1202,  -1.6122,  -0.3648,  -2.6822,   0.0319,
			                 -0.8301,  -1.3508,  -1.1312,  -1.3306])
			Loss decreased
			Epoch 0 took 0.45s NLL: 1.3241962194 Reg.: 0.0003780853 Distance: 4.0392937660 Patience: 10
			Loss decreased
			Epoch 1 took 0.47s NLL: 1.3222550154 Reg.: 0.0004260489 Distance: 3.2133171558 Patience: 10
			Loss decreased
			Epoch 2 took 0.46s NLL: 1.3219883442 Reg.: 0.0004477386 Distance: 2.6848759651 Patience: 10
			Loss decreased
			Epoch 3 took 0.57s NLL: 1.3213230371 Reg.: 0.0002602257 Distance: 12.9209585190 Patience: 10
			Loss decreased
			Epoch 4 took 0.54s NLL: 1.2876080275 Reg.: 0.0002969088 Distance: 3.6854357719 Patience: 10
			Loss decreased
			Epoch 5 took 0.50s NLL: 1.2729550600 Reg.: 0.0002997268 Distance: 1.9017144442 Patience: 10
			Loss decreased
			Epoch 6 took 0.49s NLL: 1.2728049755 Reg.: 0.0001724742 Distance: 5.7631349564 Patience: 10
			Epoch 7 took 0.06s NLL: 1.2728050947 Reg.: 0.0001723609 Distance: 0.0093386229 Patience: 9
			Loss decreased
			Epoch 8 took 0.58s NLL: 1.2723785639 Reg.: 0.0001794007 Distance: 4.1254005432 Patience: 10
			Loss decreased
			Epoch 9 took 0.20s NLL: 1.2723627090 Reg.: 0.0001834576 Distance: 0.6995053291 Patience: 10
			Loss decreased
			Epoch 10 took 0.20s NLL: 1.2723625898 Reg.: 0.0001835115 Distance: 0.0315955430 Patience: 10
			Epoch 11 took 0.18s NLL: 1.2723625898 Reg.: 0.0001835115 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.502890586853027
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -4.10325813293457
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-0.8806, -1.2694, -0.9004, -1.0195, -0.8920, -0.7084, -0.7085, -1.7620,
			                -1.4091,  0.4081, -2.9851, -0.0807,  0.4065, -0.5954, -3.3992, -0.4816,
			                -0.7030,  1.1139, -2.2899, -2.1894,  0.3042, -2.9861, -4.5007,  3.1119,
			                -0.2912,  1.1930, -6.6605,  1.6901, -0.6291, -1.2893, -0.9181, -1.2355]) 

