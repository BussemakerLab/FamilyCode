### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 3.3402030468 Reg.: 0.0016917270 Distance: 1.4224557877 Patience: 10
			Epoch 1 took 0.01s NLL: 3.3402030468 Reg.: 0.0016917270 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.005974769592285
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -6.61541223526001
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -5.7060418128967285
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.28s NLL: 1.7618144751 Reg.: 0.0010884536 Distance: 27.6118850708 Patience: 10
			Loss decreased
			Epoch 1 took 0.28s NLL: 1.6526895761 Reg.: 0.0010011776 Distance: 5.9133744240 Patience: 10
			Loss decreased
			Epoch 2 took 0.24s NLL: 1.6501542330 Reg.: 0.0009792279 Distance: 1.6404650211 Patience: 10
			Loss decreased
			Epoch 3 took 0.08s NLL: 1.6501499414 Reg.: 0.0009780283 Distance: 0.0554619804 Patience: 10
			Epoch 4 took 0.03s NLL: 1.6501499414 Reg.: 0.0009779930 Distance: 0.0008779241 Patience: 9
			Loss decreased
			Epoch 5 took 0.28s NLL: 1.6496239901 Reg.: 0.0006004719 Distance: 7.7698559761 Patience: 10
			Loss decreased
			Epoch 6 took 0.30s NLL: 1.6460602283 Reg.: 0.0004633288 Distance: 13.8771810532 Patience: 10
			Loss decreased
			Epoch 7 took 0.30s NLL: 1.6415885687 Reg.: 0.0004556360 Distance: 5.8500328064 Patience: 10
			Loss decreased
			Epoch 8 took 0.29s NLL: 1.6413859129 Reg.: 0.0003496784 Distance: 5.0079832077 Patience: 10
			Loss decreased
			Epoch 9 took 0.14s NLL: 1.6413542032 Reg.: 0.0003520080 Distance: 0.1766553074 Patience: 10
			Epoch 10 took 0.08s NLL: 1.6413542032 Reg.: 0.0003520080 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -6.61541223526001
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -5.7060418128967285
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.5685, -1.9631, -1.5983, -2.1060, -2.4213, -1.2821, -2.6391, -0.8933,
			                -1.9686, -2.4331, -3.8899,  1.0549,  1.1488, -3.0327, -2.6591, -2.6937,
			                 5.3012, -3.7819, -0.1248, -8.6297, -3.6688, -3.4668, -2.4772,  2.3758,
			                -2.2141, -3.1338, -2.3270,  0.4393, -0.1938, -3.7496, -0.5904, -2.7018]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.61541223526001
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -5.7060418128967285
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.5685, -1.9631, -1.5983, -2.1060, -2.4213, -1.2821, -2.6391, -0.8933,
			                -1.9686, -2.4331, -3.8899,  1.0549,  1.1488, -3.0327, -2.6591, -2.6937,
			                 5.3012, -3.7819, -0.1248, -8.6297, -3.6688, -3.4668, -2.4772,  2.3758,
			                -2.2141, -3.1338, -2.3270,  0.4393, -0.1938, -3.7496, -0.5904, -2.7018])
			Loss decreased
			Epoch 0 took 0.26s NLL: 1.6303281784 Reg.: 0.0003336378 Distance: 1.1357343197 Patience: 10
			Loss decreased
			Epoch 1 took 0.12s NLL: 1.6303222179 Reg.: 0.0003334235 Distance: 0.0414983630 Patience: 10
			Loss decreased
			Epoch 2 took 0.07s NLL: 1.6303218603 Reg.: 0.0003332895 Distance: 0.0147959897 Patience: 10
			Loss decreased
			Epoch 3 took 0.11s NLL: 1.6303216219 Reg.: 0.0003332460 Distance: 0.0044820714 Patience: 10
			Epoch 4 took 0.08s NLL: 1.6303216219 Reg.: 0.0003332460 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -7.040969371795654
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -5.332437038421631
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.4935, -1.8524, -1.5134, -1.9997, -2.2882, -1.2038, -2.5058, -0.8611,
			                -1.9294, -2.3112, -3.6011,  0.9820,  1.0459, -2.8708, -2.5061, -2.5289,
			                 5.3221, -3.7832,  0.2140, -8.6111, -3.2935, -3.1114, -2.4210,  1.9658,
			                -2.0473, -3.0074, -2.2129,  0.4089, -0.2433, -3.5665, -0.5873, -2.4616]) 

### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Exponential Bound: 40
	 Excluded Reg.: ()
	 Eq. Contribution: False
	 Weights: [1.0]

### Transforms:
	BoundUnsaturatedRound-0

### Binding:
	 Mode 0: NonSpecific-NS
	 Mode 1: PSAM( kernel_size=8, alphabet=DNA() )

### Tables:
	Table: 0
		Maximum Variable Length: 36
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NonSpecific-NS
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(contribution=Contribution( Mode-NS ))
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -3.5835189819335938
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 3.3402030468 Reg.: 0.0016917270 Distance: 1.4224557877 Patience: 10
			Epoch 1 took 0.01s NLL: 3.3402030468 Reg.: 0.0016917270 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -5.005974769592285
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -inf
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: PSAM( kernel_size=8, alphabet=DNA() )
	0.	MultiRoundMSLELoss.freeze()
		Aggregate.activity_heuristic(k=Contribution)
		BoundUnsaturatedRound-0.unfreeze(parameter=depth)
	1.	PSAM.unfreeze(parameter=monomer)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -6.61541223526001
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -5.7060418128967285
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.29s NLL: 1.7618144751 Reg.: 0.0010884536 Distance: 27.6118850708 Patience: 10
			Loss decreased
			Epoch 1 took 0.26s NLL: 1.6526895761 Reg.: 0.0010011776 Distance: 5.9133744240 Patience: 10
			Loss decreased
			Epoch 2 took 0.25s NLL: 1.6501542330 Reg.: 0.0009792279 Distance: 1.6404650211 Patience: 10
			Loss decreased
			Epoch 3 took 0.08s NLL: 1.6501499414 Reg.: 0.0009780283 Distance: 0.0554619804 Patience: 10
			Epoch 4 took 0.03s NLL: 1.6501499414 Reg.: 0.0009779930 Distance: 0.0008779241 Patience: 9
			Loss decreased
			Epoch 5 took 0.29s NLL: 1.6496239901 Reg.: 0.0006004719 Distance: 7.7698559761 Patience: 10
			Loss decreased
			Epoch 6 took 0.29s NLL: 1.6460602283 Reg.: 0.0004633288 Distance: 13.8771810532 Patience: 10
			Loss decreased
			Epoch 7 took 0.29s NLL: 1.6415885687 Reg.: 0.0004556360 Distance: 5.8500328064 Patience: 10
			Loss decreased
			Epoch 8 took 0.30s NLL: 1.6413859129 Reg.: 0.0003496784 Distance: 5.0079832077 Patience: 10
			Loss decreased
			Epoch 9 took 0.14s NLL: 1.6413542032 Reg.: 0.0003520080 Distance: 0.1766553074 Patience: 10
			Epoch 10 took 0.08s NLL: 1.6413542032 Reg.: 0.0003520080 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=False -6.61541223526001
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=False -5.7060418128967285
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.5685, -1.9631, -1.5983, -2.1060, -2.4213, -1.2821, -2.6391, -0.8933,
			                -1.9686, -2.4331, -3.8899,  1.0549,  1.1488, -3.0327, -2.6591, -2.6937,
			                 5.3012, -3.7819, -0.1248, -8.6297, -3.6688, -3.4668, -2.4772,  2.3758,
			                -2.2141, -3.1338, -2.3270,  0.4393, -0.1938, -3.7496, -0.5904, -2.7018]) 

	2.	MultiRoundMSLELoss.unfreeze(parameter=all)
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -6.61541223526001
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -5.7060418128967285
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.5685, -1.9631, -1.5983, -2.1060, -2.4213, -1.2821, -2.6391, -0.8933,
			                -1.9686, -2.4331, -3.8899,  1.0549,  1.1488, -3.0327, -2.6591, -2.6937,
			                 5.3012, -3.7819, -0.1248, -8.6297, -3.6688, -3.4668, -2.4772,  2.3758,
			                -2.2141, -3.1338, -2.3270,  0.4393, -0.1938, -3.7496, -0.5904, -2.7018])
			Loss decreased
			Epoch 0 took 0.26s NLL: 1.6303281784 Reg.: 0.0003336378 Distance: 1.1357343197 Patience: 10
			Loss decreased
			Epoch 1 took 0.11s NLL: 1.6303222179 Reg.: 0.0003334235 Distance: 0.0414983630 Patience: 10
			Loss decreased
			Epoch 2 took 0.06s NLL: 1.6303218603 Reg.: 0.0003332895 Distance: 0.0147959897 Patience: 10
			Loss decreased
			Epoch 3 took 0.09s NLL: 1.6303216219 Reg.: 0.0003332460 Distance: 0.0044820714 Patience: 10
			Epoch 4 took 0.08s NLL: 1.6303216219 Reg.: 0.0003332460 Distance: 0.0000000000 Patience: 9
				transforms.0.log_depth grad=False 0.0
				transforms.0.aggregate.log_target_concentration grad=False 0.0
				transforms.0.aggregate.contributions.0.log_activity grad=True -7.040969371795654
				transforms.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				transforms.0.aggregate.contributions.1.log_activity grad=True -5.332437038421631
				transforms.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				transforms.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
			                 [0., 0., 0.,  ..., 0., 0., 0.]]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				transforms.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([-1.4935, -1.8524, -1.5134, -1.9997, -2.2882, -1.2038, -2.5058, -0.8611,
			                -1.9294, -2.3112, -3.6011,  0.9820,  1.0459, -2.8708, -2.5061, -2.5289,
			                 5.3221, -3.7832,  0.2140, -8.6111, -3.2935, -3.1114, -2.4210,  1.9658,
			                -2.0473, -3.0074, -2.2129,  0.4089, -0.2433, -3.5665, -0.5873, -2.4616]) 

