### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Pseudocount: 20
	 Exponential Bound: 40
	 Excluded Reg.: frozenset()
	 Eq. Contribution: False
	 Weights: [1.0]

### Binding Components:
	 Mode 0: (NSNonSpecific,)
		0thBoundUnsaturatedRound→Aggregate→0thContribution
	 Mode 1: (0thPSAM,)
		0thBoundUnsaturatedRound→Aggregate→1stContribution

### Tables:
	Table: 0
		Maximum Variable Length: 8
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NSNonSpecific
	MultiExperimentLogMSELoss → 0thBoundUnsaturatedRound → 0thBoundUnsaturatedRound→Aggregate → 0thBoundUnsaturatedRound→Aggregate→0thContribution → 0thBoundUnsaturatedRound→NSNonSpecificMode
	0.	MultiExperimentLogMSELoss.freeze()
		0thBoundUnsaturatedRound→Aggregate.activity_heuristic(contribution=0thBoundUnsaturatedRound→Aggregate→0thContribution)
		0thBoundUnsaturatedRound.unfreeze(parameter=depth)
	1.	MultiExperimentLogMSELoss.unfreeze(parameter=all)
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -2.079441547393799
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -inf
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.5111814737 Reg.: 0.0016734631 Distance: 0.5274372101 Patience: 10
			Epoch 1 took 0.01s NLL: 1.5111814737 Reg.: 0.0016734631 Distance: 0.0000000000 Patience: 9
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -2.6068787574768066
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -inf
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: 0thPSAM
	MultiExperimentLogMSELoss → 0thBoundUnsaturatedRound → 0thBoundUnsaturatedRound→Aggregate → 0thBoundUnsaturatedRound→Aggregate→1stContribution → 0thBoundUnsaturatedRound→0thPSAMMode
	0.	MultiExperimentLogMSELoss.freeze()
		0thBoundUnsaturatedRound→Aggregate.activity_heuristic(contribution=0thBoundUnsaturatedRound→Aggregate→1stContribution)
		0thBoundUnsaturatedRound.unfreeze(parameter=depth)
	1.	0thPSAM.unfreeze(parameter=monomer)
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=False -4.216316223144531
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -1.443727970123291
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.29s NLL: 2.8523266315 Reg.: 0.0006461478 Distance: 29.3978996277 Patience: 10
			Loss decreased
			Epoch 1 took 0.35s NLL: 2.0670459270 Reg.: 0.0006233937 Distance: 3.5807564259 Patience: 10
			Epoch 2 took 0.02s NLL: 2.0670459270 Reg.: 0.0006233705 Distance: 0.0005154173 Patience: 9
			Loss decreased
			Epoch 3 took 0.03s NLL: 2.0670461655 Reg.: 0.0006229418 Distance: 0.0088225724 Patience: 10
			Loss decreased
			Epoch 4 took 0.26s NLL: 1.0988287926 Reg.: 0.0000253745 Distance: 23.3772773743 Patience: 10
			Epoch 5 took 0.08s NLL: 1.0988287926 Reg.: 0.0000253745 Distance: 0.0000000000 Patience: 9
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=False -4.216316223144531
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -1.443727970123291
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ 0.2368, -0.4117, -0.4513,  0.2661,  0.1379, -0.3612, -0.5416,  0.4048,
			                -0.0473, -0.2541, -0.7329,  0.6750,  0.4743, -0.5806, -0.4159,  0.1616,
			                 0.3679, -0.5535, -0.3155,  0.1398, -0.3816, -0.1114, -0.6056,  0.7401,
			                 0.2561, -0.4064, -0.4409,  0.2310,  0.4087, -0.4068, -0.3756,  0.0139]) 

	2.	MultiExperimentLogMSELoss.unfreeze(parameter=all)
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -4.216316223144531
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=True -1.443727970123291
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ 0.2368, -0.4117, -0.4513,  0.2661,  0.1379, -0.3612, -0.5416,  0.4048,
			                -0.0473, -0.2541, -0.7329,  0.6750,  0.4743, -0.5806, -0.4159,  0.1616,
			                 0.3679, -0.5535, -0.3155,  0.1398, -0.3816, -0.1114, -0.6056,  0.7401,
			                 0.2561, -0.4064, -0.4409,  0.2310,  0.4087, -0.4068, -0.3756,  0.0139])
			Loss decreased
			Epoch 0 took 0.22s NLL: 1.0509406328 Reg.: 0.0000273144 Distance: 2.2781689167 Patience: 10
			Loss decreased
			Epoch 1 took 0.19s NLL: 1.0436104536 Reg.: 0.0000277825 Distance: 0.7047986984 Patience: 10
			Epoch 2 took 0.02s NLL: 1.0436104536 Reg.: 0.0000277833 Distance: 0.0005322250 Patience: 9
			Epoch 3 took 0.02s NLL: 1.0436104536 Reg.: 0.0000277830 Distance: 0.0007080223 Patience: 8
			Loss decreased
			Epoch 4 took 0.03s NLL: 1.0436103344 Reg.: 0.0000277822 Distance: 0.0003726438 Patience: 10
			Epoch 5 took 0.01s NLL: 1.0436103344 Reg.: 0.0000277822 Distance: 0.0000000000 Patience: 9
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -3.6880218982696533
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=True -1.7107698917388916
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ 0.0146, -0.2976, -0.4020,  0.0552, -0.0999, -0.1905, -0.3594,  0.0199,
			                 0.0158, -0.2882, -0.3974,  0.0408,  0.2985, -0.5492, -0.5545,  0.1748,
			                 0.5482, -0.8123, -0.7249,  0.3579,  0.7400, -0.9853, -1.0977,  0.7147,
			                 0.7473, -0.9668, -1.1006,  0.6902,  0.5638, -0.7919, -0.8325,  0.4311]) 

