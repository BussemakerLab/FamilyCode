### Regularization:
	 L1 Lambda: 0
	 L2 Lambda: 1e-06
	 Pseudocount: 20
	 Exponential Bound: 40
	 Excluded Reg.: frozenset()
	 Eq. Contribution: False
	 Weights: [1.0]

### Binding Components:
	 Mode 0: (NSNonSpecific,)
		0thBoundUnsaturatedRound→Aggregate→0thContribution
	 Mode 1: (0thPSAM,)
		0thBoundUnsaturatedRound→Aggregate→1stContribution

### Tables:
	Table: 0
		Maximum Variable Length: 8
		Left Flank Length: 0
		Right Flank Length: 0

### Training Mode 0: NSNonSpecific
	MultiExperimentLogMSELoss → 0thBoundUnsaturatedRound → 0thBoundUnsaturatedRound→Aggregate → 0thBoundUnsaturatedRound→Aggregate→0thContribution → 0thBoundUnsaturatedRound→NSNonSpecificMode
	0.	MultiExperimentLogMSELoss.freeze()
		0thBoundUnsaturatedRound→Aggregate.activity_heuristic(contribution=0thBoundUnsaturatedRound→Aggregate→0thContribution)
		0thBoundUnsaturatedRound.unfreeze(parameter=depth)
	1.	MultiExperimentLogMSELoss.unfreeze(parameter=all)
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -2.079441547393799
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -inf
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.02s NLL: 1.6398472786 Reg.: 0.0016730990 Distance: 0.4566526413 Patience: 10
			Epoch 1 took 0.01s NLL: 1.6398472786 Reg.: 0.0016730990 Distance: 0.0000000000 Patience: 9
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -2.5360941886901855
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -inf
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=False
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000]) 


### Training Mode 1: 0thPSAM
	MultiExperimentLogMSELoss → 0thBoundUnsaturatedRound → 0thBoundUnsaturatedRound→Aggregate → 0thBoundUnsaturatedRound→Aggregate→1stContribution → 0thBoundUnsaturatedRound→0thPSAMMode
	0.	MultiExperimentLogMSELoss.freeze()
		0thBoundUnsaturatedRound→Aggregate.activity_heuristic(contribution=0thBoundUnsaturatedRound→Aggregate→1stContribution)
		0thBoundUnsaturatedRound.unfreeze(parameter=depth)
	1.	0thPSAM.unfreeze(parameter=monomer)
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=False -4.14553165435791
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -1.37294340133667
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,  -5.8926,  -5.8926,  -5.8926,  17.6777,   5.8926, -17.6777,
			                  5.8926,   5.8926,  17.6777,  -5.8926,  -5.8926,  -5.8926, -10.2062,
			                 10.2062, -10.2062,  10.2062,   0.0000,   0.0000,   0.0000,   0.0000,
			                  0.0000,   0.0000,   0.0000,   0.0000])
			Loss decreased
			Epoch 0 took 0.28s NLL: 2.8061985970 Reg.: 0.0006826887 Distance: 29.1443138123 Patience: 10
			Loss decreased
			Epoch 1 took 0.21s NLL: 2.1602220535 Reg.: 0.0006650403 Distance: 3.1211574078 Patience: 10
			Loss decreased
			Epoch 2 took 0.05s NLL: 2.1601009369 Reg.: 0.0006647815 Distance: 0.0558436401 Patience: 10
			Epoch 3 took 0.02s NLL: 2.1601009369 Reg.: 0.0006647679 Distance: 0.0007943872 Patience: 9
			Epoch 4 took 0.02s NLL: 2.1601009369 Reg.: 0.0006647464 Distance: 0.0004932408 Patience: 8
			Epoch 5 took 0.02s NLL: 2.1601009369 Reg.: 0.0006646202 Distance: 0.0025768708 Patience: 7
			Loss decreased
			Epoch 6 took 0.26s NLL: 1.1778727770 Reg.: 0.0000263049 Distance: 24.1069908142 Patience: 10
			Epoch 7 took 0.12s NLL: 1.1778727770 Reg.: 0.0000263049 Distance: 0.0000000000 Patience: 9
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=False -4.14553165435791
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=False -1.37294340133667
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ 0.3250, -0.3504, -0.4484,  0.0072,  0.0959, -0.3866, -0.6444,  0.4670,
			                -0.1450, -0.1954, -0.8648,  0.7384,  0.5703, -0.6170, -0.3772, -0.0428,
			                 0.5464, -0.8192, -0.2788,  0.0849, -0.4465, -0.1279, -0.6485,  0.7552,
			                 0.3399, -0.6097, -0.4428,  0.2470,  0.5848, -0.5866, -0.3074, -0.1567]) 

	2.	MultiExperimentLogMSELoss.unfreeze(parameter=all)
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -4.14553165435791
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=True -1.37294340133667
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ 0.3250, -0.3504, -0.4484,  0.0072,  0.0959, -0.3866, -0.6444,  0.4670,
			                -0.1450, -0.1954, -0.8648,  0.7384,  0.5703, -0.6170, -0.3772, -0.0428,
			                 0.5464, -0.8192, -0.2788,  0.0849, -0.4465, -0.1279, -0.6485,  0.7552,
			                 0.3399, -0.6097, -0.4428,  0.2470,  0.5848, -0.5866, -0.3074, -0.1567])
			Loss decreased
			Epoch 0 took 0.14s NLL: 1.1370600462 Reg.: 0.0000260808 Distance: 0.6236513853 Patience: 10
			Epoch 1 took 0.02s NLL: 1.1370600462 Reg.: 0.0000260811 Distance: 0.0005770720 Patience: 9
			Epoch 2 took 0.02s NLL: 1.1370600462 Reg.: 0.0000260811 Distance: 0.0001248709 Patience: 8
			Epoch 3 took 0.02s NLL: 1.1370600462 Reg.: 0.0000260809 Distance: 0.0001467181 Patience: 7
			Epoch 4 took 0.01s NLL: 1.1370600462 Reg.: 0.0000260809 Distance: 0.0000000000 Patience: 6
				rounds.0.log_depth grad=False 0.0
				rounds.0.aggregate.log_target_concentration grad=False 0.0
				rounds.0.aggregate.contributions.0.log_activity grad=True -4.086243629455566
				rounds.0.aggregate.contributions.0.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.0.binding.layers.0.log_posbias grad=False tensor([[[0.]]])
				rounds.0.aggregate.contributions.1.log_activity grad=True -1.4158860445022583
				rounds.0.aggregate.contributions.1.binding.log_hill grad=False 0.0
				rounds.0.aggregate.contributions.1.binding.layers.0.log_posbias grad=False
					tensor([[[0.],
			                 [0.]]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.bias grad=False tensor([[0.]])
				rounds.0.aggregate.contributions.1.binding.layers.0.layer_spec.betas-monomer grad=True
					tensor([ 0.5346, -0.4396, -0.3698, -0.2349,  0.1626, -0.4502, -0.6658,  0.4423,
			                -0.3030, -0.2829, -0.7410,  0.8169,  0.5942, -0.6212, -0.3853, -0.0976,
			                 0.6784, -0.7344, -0.3464, -0.1075, -0.3155, -0.3431, -0.5500,  0.6978,
			                 0.1508, -0.5038, -0.4749,  0.3191,  0.5800, -0.5918, -0.2842, -0.2130]) 

